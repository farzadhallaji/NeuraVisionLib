{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/NeuraVisionLib/')\n",
    "# ! git pull\n",
    "\n",
    "while not 'dataloaders' in os.listdir():\n",
    "    os.chdir('../')\n",
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('../../../MyDrive/mass_dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.errors import NotGeoreferencedWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet50\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "from dataloaders.mass_roads_dataloader import MassRoadsDataset, custom_collate_fn\n",
    "from models.MULDE.models import MLPs, ScoreOrLogDensityNetwork\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "# DATA_DIR = '/home/ri/Desktop/Projects/Datasets/Mass_Roads/dataset/'\n",
    "DATA_DIR = '../../../MyDrive/mass_dataset/'\n",
    "CHECKPOINT_PATH = 'log_density_segmentation_checkpoint.pth'\n",
    "BEST_MODEL_PATH = 'best_log_density_segmentation_model.pth'\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 300\n",
    "ACCUMULATION_STEPS = 8\n",
    "WINDOW_SIZE = 128\n",
    "WINDOW_STRIDE = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SIGMA_MIN = 0.01\n",
    "SIGMA_MAX = 0.3\n",
    "NUM_NOISE_LEVELS = 10\n",
    "USE_AUTOMATIC_MIXED_PRECISION = True  # Set to True to use torch.cuda.amp.autocast\n",
    "\n",
    "# Generate noise levels\n",
    "noise_levels = torch.logspace(\n",
    "    start=torch.log10(torch.tensor(SIGMA_MIN)),\n",
    "    end=torch.log10(torch.tensor(SIGMA_MAX)),\n",
    "    steps=NUM_NOISE_LEVELS\n",
    ").to(DEVICE)\n",
    "\n",
    "# --- Dataset Preparation ---\n",
    "train_dataset = MassRoadsDataset(root_dir=DATA_DIR, split='train', window_size=WINDOW_SIZE, stride=WINDOW_STRIDE)\n",
    "val_dataset = MassRoadsDataset(root_dir=DATA_DIR, split='val', window_size=WINDOW_SIZE, stride=WINDOW_STRIDE)\n",
    "test_dataset = MassRoadsDataset(root_dir=DATA_DIR, split='test', window_size=WINDOW_SIZE, stride=WINDOW_STRIDE)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "def plot_patches(sat_patches, map_patches, n_patches=1):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    for i in range(min(n_patches, sat_patches.shape[0])):\n",
    "        sat_patch = sat_patches[i].numpy().transpose(1, 2, 0) \n",
    "        map_patch = map_patches[i].numpy()\n",
    "\n",
    "        ax[0].imshow(sat_patch)\n",
    "        ax[0].set_title(f'Satellite Patch {i+1}')\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        ax[1].imshow(map_patch, cmap='gray')\n",
    "        ax[1].set_title(f'Map Patch {i+1}')\n",
    "        ax[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for i, (sat_patches, map_patches) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"Sat Patches Shape: {sat_patches.shape}\")\n",
    "    print(f\"Map Patches Shape: {map_patches.shape}\")\n",
    "\n",
    "    plot_patches(sat_patches, map_patches, n_patches=1)\n",
    "    if i > 2:\n",
    "        break\n",
    "\n",
    "# --- Model Definition ---\n",
    "resnet = resnet50(pretrained=True)\n",
    "resnet = nn.Sequential(\n",
    "    *list(resnet.children())[:-2],\n",
    "    nn.AdaptiveAvgPool2d((1, 1))\n",
    ").to(DEVICE)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "mlp = MLPs(\n",
    "    input_dim=2048 + 1,\n",
    "    output_dim=WINDOW_SIZE * WINDOW_SIZE,\n",
    "    units=[4096, 4096],\n",
    "    layernorm=True,\n",
    "    dropout=0.1\n",
    ")\n",
    "log_density_model = ScoreOrLogDensityNetwork(mlp, score_network=False).to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(log_density_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "scaler = torch.cuda.amp.GradScaler() if USE_AUTOMATIC_MIXED_PRECISION else None\n",
    "\n",
    "# --- Loss Functions ---\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    intersection = (pred * target).sum()\n",
    "    return 1 - (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "\n",
    "def combined_loss(pred, target):\n",
    "    bce = nn.BCELoss()(pred, target)\n",
    "    dice = dice_loss(pred, target)\n",
    "    return bce + dice\n",
    "\n",
    "# --- Save and Load Model ---\n",
    "def save_model(log_density_model, optimizer, epoch, loss, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': log_density_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, path)\n",
    "    print(f\"Model saved at {path}\")\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        log_density_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        return checkpoint['epoch'], checkpoint['loss']\n",
    "    return 0, None\n",
    "\n",
    "# --- Training and Validation Loops ---\n",
    "def forward_pass(satellite_patches, noise_levels, resnet, log_density_model):\n",
    "    features = resnet(satellite_patches).flatten(start_dim=1)\n",
    "    batch_size = features.size(0)\n",
    "    num_noise_levels = len(noise_levels)\n",
    "    features = features.repeat_interleave(num_noise_levels, dim=0)\n",
    "    noise_tensor = noise_levels.repeat(batch_size, 1).view(-1, 1)\n",
    "    features_with_noise = torch.cat([features, noise_tensor], dim=1)\n",
    "    predictions = log_density_model(features_with_noise)\n",
    "    return predictions.view(batch_size, num_noise_levels, 1, WINDOW_SIZE, WINDOW_SIZE)\n",
    "\n",
    "def train_one_epoch(train_loader, resnet, log_density_model, noise_levels, optimizer, device, scaler, accumulation_steps):\n",
    "    resnet.eval()\n",
    "    log_density_model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, (satellite_patches, road_maps) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        if satellite_patches.numel() == 0 or road_maps.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        satellite_patches = satellite_patches.to(device)\n",
    "        road_maps = road_maps.to(device).float() / 255.0\n",
    "        road_maps = road_maps.unsqueeze(1)\n",
    "\n",
    "        if USE_AUTOMATIC_MIXED_PRECISION:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions = forward_pass(satellite_patches, noise_levels, resnet, log_density_model)\n",
    "                predictions = predictions[:, 0, :, :, :]\n",
    "                loss = combined_loss(torch.sigmoid(predictions), road_maps)\n",
    "                loss = loss / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            predictions = forward_pass(satellite_patches, noise_levels, resnet, log_density_model)\n",
    "            predictions = predictions[:, 0, :, :, :]\n",
    "            loss = combined_loss(torch.sigmoid(predictions), road_maps)\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            if USE_AUTOMATIC_MIXED_PRECISION:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "def validate_one_epoch(val_loader, resnet, log_density_model, noise_levels, device, scaler):\n",
    "    resnet.eval()\n",
    "    log_density_model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for satellite_patches, road_maps in tqdm(val_loader, desc=\"Validation\"):\n",
    "            if satellite_patches.numel() == 0 or road_maps.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            satellite_patches = satellite_patches.to(device)\n",
    "            road_maps = road_maps.to(device).float() / 255.0\n",
    "            road_maps = road_maps.unsqueeze(1)\n",
    "\n",
    "            if USE_AUTOMATIC_MIXED_PRECISION:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    predictions = forward_pass(satellite_patches, noise_levels, resnet, log_density_model)\n",
    "                    predictions = predictions[:, 0, :, :, :]\n",
    "                    loss = combined_loss(torch.sigmoid(predictions), road_maps)\n",
    "            else:\n",
    "                predictions = forward_pass(satellite_patches, noise_levels, resnet, log_density_model)\n",
    "                predictions = predictions[:, 0, :, :, :]\n",
    "                loss = combined_loss(torch.sigmoid(predictions), road_maps)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "def train_model(resnet, log_density_model, train_loader, val_loader, noise_levels, optimizer, scheduler, device, epochs, accumulation_steps, checkpoint_path, best_model_path):\n",
    "    start_epoch, _ = load_checkpoint(checkpoint_path)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        train_loss = train_one_epoch(train_loader, resnet, log_density_model, noise_levels, optimizer, device, scaler, accumulation_steps)\n",
    "        val_loss = validate_one_epoch(val_loader, resnet, log_density_model, noise_levels, device, scaler)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "        save_model(log_density_model, optimizer, epoch, val_loss, path=checkpoint_path)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(log_density_model, optimizer, epoch, val_loss, path=best_model_path)\n",
    "            print(f\"Best model updated with Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate(model, resnet, dataloader, noise_levels, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions_list = []\n",
    "    ground_truth_list = []\n",
    "    satellite_patches_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for satellite_patches, road_maps in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            if satellite_patches.numel() == 0 or road_maps.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            satellite_patches = satellite_patches.to(device)\n",
    "            road_maps = road_maps.to(device).float() / 255.0\n",
    "\n",
    "            if USE_AUTOMATIC_MIXED_PRECISION:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    predictions = forward_pass(satellite_patches, noise_levels, resnet, model)\n",
    "                    predictions = predictions[:, 0, :, :, :]  # Use the predictions for noise level 0\n",
    "            else:\n",
    "                predictions = forward_pass(satellite_patches, noise_levels, resnet, model)\n",
    "                predictions = predictions[:, 0, :, :, :]  # Use the predictions for noise level 0\n",
    "\n",
    "            predictions_list.append(torch.sigmoid(predictions).cpu())\n",
    "            ground_truth_list.append(road_maps.cpu())\n",
    "            satellite_patches_list.append(satellite_patches.cpu())\n",
    "\n",
    "            # Compute loss\n",
    "            loss = combined_loss(torch.sigmoid(predictions), road_maps)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Compute metrics\n",
    "    all_preds = torch.cat([p.flatten() for p in predictions_list]).numpy() > 0.5\n",
    "    all_targets = torch.cat([t.flatten() for t in ground_truth_list]).numpy()\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    recall = recall_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    iou = jaccard_score(all_targets, all_preds)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, IoU: {iou:.4f}\")\n",
    "\n",
    "    return total_loss / len(dataloader), predictions_list, ground_truth_list, satellite_patches_list\n",
    "\n",
    "# --- Plot Predictions and Ground Truth ---\n",
    "def plot_predictions(predictions, ground_truth, satellite_images, n_samples=5):\n",
    "    for i in range(min(n_samples, len(predictions))):\n",
    "        pred = predictions[i].squeeze().numpy()\n",
    "        gt = ground_truth[i].squeeze().numpy()\n",
    "        sat_img = satellite_images[i].permute(1, 2, 0).numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        axes[0].imshow(sat_img)\n",
    "        axes[0].set_title('Satellite Image')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(pred, cmap='gray')\n",
    "        axes[1].set_title('Prediction')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        axes[2].imshow(gt, cmap='gray')\n",
    "        axes[2].set_title('Ground Truth')\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Start Training ---\n",
    "train_model(\n",
    "    resnet=resnet,\n",
    "    log_density_model=log_density_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    noise_levels=noise_levels,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    epochs=EPOCHS,\n",
    "    accumulation_steps=ACCUMULATION_STEPS,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    best_model_path=BEST_MODEL_PATH,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate on Test Set ---\n",
    "val_loss, predictions, ground_truth, satellite_patches = evaluate(\n",
    "    model=log_density_model,\n",
    "    resnet=resnet,\n",
    "    dataloader=test_loader,\n",
    "    noise_levels=noise_levels,\n",
    "    device=DEVICE,\n",
    ")\n",
    "print(f\"Test Loss: {val_loss:.4f}\")\n",
    "\n",
    "# --- Plot Predictions ---\n",
    "plot_predictions(predictions, ground_truth, satellite_patches, n_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/300: 100%|██████████| 277/277 [53:31<00:00, 11.59s/it]\n",
    "# Epoch 1/300, Loss: 0.1431\n",
    "# Checkpoint saved at log_density_segmentation_checkpoint.pth\n",
    "# Epoch 2/300: 100%|██████████| 277/277 [27:57<00:00,  6.06s/it]\n",
    "# Epoch 2/300, Loss: 0.1259\n",
    "# Checkpoint saved at log_density_segmentation_checkpoint.pth\n",
    "# Epoch 3/300: 100%|██████████| 277/277 [27:26<00:00,  5.94s/it]\n",
    "# Epoch 3/300, Loss: 0.1256\n",
    "# Checkpoint saved at log_density_segmentation_checkpoint.pth\n",
    "# Epoch 4/300: 100%|██████████| 277/277 [26:52<00:00,  5.82s/it]\n",
    "# Epoch 4/300, Loss: 0.1255\n",
    "# Checkpoint saved at log_density_segmentation_checkpoint.pth\n",
    "# Epoch 5/300: 100%|██████████| 277/277 [27:13<00:00,  5.90s/it]\n",
    "# Epoch 5/300, Loss: 0.1254\n",
    "# Checkpoint saved at log_density_segmentation_checkpoint.pth\n",
    "# Epoch 6/300: 100%|██████████| 277/277 [27:23<00:00,  5.93s/it]\n",
    "# Epoch 6/300, Loss: 0.1254\n",
    "# Checkpoint saved at log_density_segmentation_checkpoint.pth\n",
    "# Epoch 7/300: 100%|██████████| 277/277 [27:14<00:00,  5.90s/it]\n",
    "# Epoch 7/300, Loss: 0.1254\n",
    "# Checkpoint saved at log_density_segmentation_checkpoint.pth\n",
    "# Epoch 8/300: 100%|██████████| 277/277 [1:03:25<00:00, 13.74s/it]\n",
    "# Epoch 8/300, Loss: 0.1254\n",
    "# Checkpoint saved at log_density_segmentation_checkpoint.pth\n",
    "# Epoch 9/300: 100%|██████████| 277/277 [28:32<00:00,  6.18s/it]\n",
    "# Epoch 9/300, Loss: 0.1254\n",
    "# Checkpoint saved at log_density_segmentation_checkpoint.pth\n",
    "# Epoch 10/300: 100%|██████████| 277/277 [28:31<00:00,  6.18s/it]\n",
    "# Epoch 10/300, Loss: 0.1254\n",
    "# Checkpoint saved at log_density_segmentation_checkpoint.pth\n",
    "# Epoch 11/300: 100%|██████████| 277/277 [28:24<00:00,  6.15s/it]\n",
    "# Epoch 11/300, Loss: 0.1254\n",
    "# Checkpoint saved at log_density_segmentation_checkpoint.pth\n",
    "# Epoch 12/300: 100%|██████████| 277/277 [28:17<00:00,  6.13s/it]\n",
    "# Epoch 12/300, Loss: 0.1254\n",
    "# Checkpoint saved at log_density_segmentation_checkpoint.pth\n",
    "# Epoch 13/300: 100%|██████████| 277/277 [28:30<00:00,  6.18s/it]\n",
    "# Epoch 13/300, Loss: 0.1254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
