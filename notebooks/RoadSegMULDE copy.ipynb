{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/NeuraVisionLib/')\n",
    "## ! git reset --hard\n",
    "# ! git pull\n",
    "\n",
    "while not 'dataloaders' in os.listdir():\n",
    "    os.chdir('../')\n",
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('../../../MyDrive/mass_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install rasterio > install.txt\n",
    "! rm -rf install.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "while not 'dataloaders' in os.listdir():\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.errors import NotGeoreferencedWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet50\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "from dataloaders.mass_roads_dataloader import MassRoadsDataset, custom_collate_fn\n",
    "from models.MULDE.models import MLPs, ScoreOrLogDensityNetwork\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "DATA_DIR = '/home/ri/Desktop/Projects/Datasets/Mass_Roads/dataset/'\n",
    "CHECKPOINT_PATH = 'checkpoints/RoadSegMulde/log_density_segmentation_checkpoint.pth'\n",
    "BEST_MODEL_PATH = 'checkpoints/RoadSegMulde/best_log_density_segmentation_model.pth'\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 300\n",
    "ACCUMULATION_STEPS = 8\n",
    "WINDOW_SIZE = 128\n",
    "WINDOW_STRIDE = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SIGMA_MIN = 0.01\n",
    "SIGMA_MAX = 0.3\n",
    "NUM_NOISE_LEVELS = 10\n",
    "USE_AUTOMATIC_MIXED_PRECISION = True\n",
    "LOAD_FROM = None #'last' # best or None\n",
    "# Generate noise levels\n",
    "noise_levels = torch.logspace(\n",
    "    start=torch.log10(torch.tensor(SIGMA_MIN)),\n",
    "    end=torch.log10(torch.tensor(SIGMA_MAX)),\n",
    "    steps=NUM_NOISE_LEVELS\n",
    ").to(DEVICE)\n",
    "\n",
    "# --- Dataset Preparation ---\n",
    "train_dataset = MassRoadsDataset(root_dir=DATA_DIR, split='train', window_size=WINDOW_SIZE, stride=WINDOW_STRIDE, max_images=3)\n",
    "val_dataset = MassRoadsDataset(root_dir=DATA_DIR, split='valid', window_size=WINDOW_SIZE, stride=WINDOW_STRIDE, max_images=3)\n",
    "test_dataset = MassRoadsDataset(root_dir=DATA_DIR, split='test', window_size=WINDOW_SIZE, stride=WINDOW_STRIDE, max_images=3)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# --- Save and Load Model ---\n",
    "def save_model(log_density_model, optimizer, epoch, loss, path):\n",
    "    directory = os.path.dirname(path)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': log_density_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, path)\n",
    "    print(f\"Model saved at {path}\")\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        log_density_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        return checkpoint['epoch'], checkpoint['loss']\n",
    "    return 0, None\n",
    "\n",
    "# --- Loss Functions ---\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Dice Loss for binary segmentation.\n",
    "    \"\"\"\n",
    "    pred = torch.sigmoid(pred)\n",
    "    intersection = (pred * target).sum()\n",
    "    dice = 1 - (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    return dice\n",
    "\n",
    "def focal_loss(logits, targets, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Focal Loss to focus on harder examples.\n",
    "    \"\"\"\n",
    "    bce = nn.BCEWithLogitsLoss(reduction='none')(logits, targets)\n",
    "    pt = torch.exp(-bce)  # Probabilities\n",
    "    focal = alpha * (1 - pt)**gamma * bce\n",
    "    return focal.mean()\n",
    "\n",
    "def weighted_bce_loss(logits, targets, pos_weight=1.0):\n",
    "    \"\"\"\n",
    "    Weighted Binary Cross Entropy to address class imbalance.\n",
    "    \"\"\"\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).to(logits.device))\n",
    "    return bce(logits, targets)\n",
    "\n",
    "def hybrid_loss(predictions, targets, alpha=0.5, beta=0.5, gamma=2.0, use_focal=True):\n",
    "    \"\"\"\n",
    "    Hybrid Loss combining Weighted BCE, Dice Loss, and optionally Focal Loss.\n",
    "    \"\"\"\n",
    "    # Calculate class weights dynamically\n",
    "    num_positive = targets.sum()\n",
    "    num_negative = targets.numel() - num_positive\n",
    "    pos_weight = num_negative / (num_positive + 1e-6)  # Avoid division by zero\n",
    "\n",
    "    # Weighted BCE Loss\n",
    "    bce = weighted_bce_loss(predictions, targets, pos_weight=pos_weight)\n",
    "\n",
    "    # Dice Loss\n",
    "    dice = dice_loss(predictions, targets)\n",
    "\n",
    "    # Focal Loss (optional)\n",
    "    focal = focal_loss(predictions, targets) if use_focal else 0\n",
    "\n",
    "    # Weighted combination of losses\n",
    "    loss = alpha * bce + beta * dice + gamma * focal\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calculate_metrics(predictions, targets, threshold=0.5):\n",
    "    preds = (predictions > threshold).float().cpu().numpy().astype(np.uint8).flatten()\n",
    "    targets = (targets > 0.5).float().cpu().numpy().astype(np.uint8).flatten()  # Ensure targets are binary\n",
    "    precision = precision_score(targets, preds, zero_division=1)\n",
    "    recall = recall_score(targets, preds, zero_division=1)\n",
    "    f1 = f1_score(targets, preds, zero_division=1)\n",
    "    iou = jaccard_score(targets, preds, zero_division=1)\n",
    "\n",
    "    return precision, recall, f1, iou\n",
    "\n",
    "# --- Model Definition ---\n",
    "resnet = resnet50(pretrained=True)\n",
    "resnet = nn.Sequential(\n",
    "    *list(resnet.children())[:-2],\n",
    "    nn.AdaptiveAvgPool2d((1, 1))\n",
    ").to(DEVICE)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "mlp = MLPs(\n",
    "    input_dim=2048 + 1,\n",
    "    output_dim=WINDOW_SIZE * WINDOW_SIZE,\n",
    "    units=[4096, 4096],\n",
    "    layernorm=True,\n",
    "    dropout=0.1\n",
    ")\n",
    "log_density_model = ScoreOrLogDensityNetwork(mlp, score_network=False).to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(log_density_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "scaler = torch.amp.GradScaler() if USE_AUTOMATIC_MIXED_PRECISION else None\n",
    "\n",
    "# --- Forward Pass ---\n",
    "def forward_pass(satellite_patches, noise_levels, resnet, log_density_model):\n",
    "    features = resnet(satellite_patches).flatten(start_dim=1)\n",
    "    batch_size = features.size(0)\n",
    "    num_noise_levels = len(noise_levels)\n",
    "    features = features.repeat_interleave(num_noise_levels, dim=0)\n",
    "    noise_tensor = noise_levels.repeat(batch_size, 1).view(-1, 1)\n",
    "    features_with_noise = torch.cat([features, noise_tensor], dim=1)\n",
    "    predictions = log_density_model(features_with_noise)\n",
    "    return predictions.view(batch_size, num_noise_levels, 1, WINDOW_SIZE, WINDOW_SIZE)\n",
    "\n",
    "# --- Training ---\n",
    "def train_one_epoch(train_loader, resnet, log_density_model, noise_levels, optimizer, device, scaler, accumulation_steps):\n",
    "    resnet.eval()\n",
    "    log_density_model.train()\n",
    "    total_loss = 0\n",
    "    total_patches = 0  # Track total number of patches\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, (satellite_patches, road_maps) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        if satellite_patches.numel() == 0 or road_maps.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        satellite_patches = satellite_patches.to(device)\n",
    "        road_maps = road_maps.to(device).float() / 255.0  # Normalize ground truth\n",
    "        road_maps = road_maps.unsqueeze(1)  # Add channel dimension\n",
    "        batch_patches = satellite_patches.size(0)  # Count patches in the batch\n",
    "\n",
    "        if USE_AUTOMATIC_MIXED_PRECISION:\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                predictions = forward_pass(satellite_patches, noise_levels, resnet, log_density_model)\n",
    "                predictions = predictions[:, 0, :, :, :]  # Use noise level 0\n",
    "                loss = hybrid_loss(predictions, road_maps) / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            predictions = forward_pass(satellite_patches, noise_levels, resnet, log_density_model)\n",
    "            predictions = predictions[:, 0, :, :, :]\n",
    "            loss = hybrid_loss(predictions, road_maps) / accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            if USE_AUTOMATIC_MIXED_PRECISION:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * batch_patches  # Scale loss by number of patches\n",
    "        total_patches += batch_patches\n",
    "\n",
    "    # Normalize loss by total patches\n",
    "    avg_loss = total_loss / total_patches\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "# --- Validation ---\n",
    "def validate_one_epoch(val_loader, resnet, log_density_model, noise_levels, device):\n",
    "    resnet.eval()\n",
    "    log_density_model.eval()\n",
    "    total_loss = 0\n",
    "    total_patches = 0\n",
    "    metrics = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for satellite_patches, road_maps in tqdm(val_loader, desc=\"Validation\"):\n",
    "            if satellite_patches.numel() == 0 or road_maps.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            satellite_patches = satellite_patches.to(device)\n",
    "            road_maps = road_maps.to(device).float() / 255.0  # Normalize ground truth\n",
    "            road_maps = road_maps.unsqueeze(1)  # Add channel dimension\n",
    "            batch_patches = satellite_patches.size(0)  # Count patches in the batch\n",
    "\n",
    "            predictions = forward_pass(satellite_patches, noise_levels, resnet, log_density_model)\n",
    "            predictions = predictions[:, 0, :, :, :]  # Use predictions for noise level 0\n",
    "            loss = hybrid_loss(predictions, road_maps)\n",
    "\n",
    "            # Threshold predictions and calculate metrics\n",
    "            precision, recall, f1, iou = calculate_metrics(torch.sigmoid(predictions), road_maps)\n",
    "            metrics.append([precision, recall, f1, iou])\n",
    "\n",
    "            total_loss += loss.item() * batch_patches  # Scale loss by number of patches\n",
    "            total_patches += batch_patches\n",
    "\n",
    "    # Normalize loss by total patches\n",
    "    avg_loss = total_loss / total_patches\n",
    "    avg_metrics = np.mean(metrics, axis=0)\n",
    "    print(f\"Validation - Precision: {avg_metrics[0]:.4f}, Recall: {avg_metrics[1]:.4f}, F1: {avg_metrics[2]:.4f}, IoU: {avg_metrics[3]:.4f}\")\n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "\n",
    "# --- Test Evaluation ---\n",
    "def evaluate_on_test(test_loader, resnet, log_density_model, noise_levels, device):\n",
    "    resnet.eval()\n",
    "    log_density_model.eval()\n",
    "    test_loss = 0\n",
    "    metrics = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for satellite_patches, road_maps in tqdm(test_loader, desc=\"Testing\"):\n",
    "            if satellite_patches.numel() == 0 or road_maps.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            satellite_patches = satellite_patches.to(device)\n",
    "            road_maps = road_maps.to(device).float() / 255.0\n",
    "            road_maps = road_maps.unsqueeze(1)\n",
    "\n",
    "            predictions = forward_pass(satellite_patches, noise_levels, resnet, log_density_model)\n",
    "            predictions = predictions[:, 0, :, :, :]\n",
    "            loss = hybrid_loss(predictions, road_maps)\n",
    "\n",
    "            precision, recall, f1, iou = calculate_metrics(torch.sigmoid(predictions), road_maps)\n",
    "            metrics.append([precision, recall, f1, iou])\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    avg_metrics = np.mean(metrics, axis=0)\n",
    "    print(f\"Test - Precision: {avg_metrics[0]:.4f}, Recall: {avg_metrics[1]:.4f}, F1: {avg_metrics[2]:.4f}, IoU: {avg_metrics[3]:.4f}\")\n",
    "    return test_loss / len(test_loader), avg_metrics\n",
    "\n",
    "# --- Plot Predictions ---\n",
    "def plot_predictions(predictions, ground_truth, satellite_images, n_samples=5):\n",
    "    for i in range(min(n_samples, len(predictions))):\n",
    "        pred = predictions[i].squeeze().cpu().numpy()\n",
    "        gt = ground_truth[i].squeeze().cpu().numpy()\n",
    "        sat_img = satellite_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        axes[0].imshow(sat_img)\n",
    "        axes[0].set_title('Satellite Image')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(pred, cmap='gray')\n",
    "        axes[1].set_title('Prediction')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        axes[2].imshow(gt, cmap='gray')\n",
    "        axes[2].set_title('Ground Truth')\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "def train_model(resnet, log_density_model, train_loader, val_loader, noise_levels, optimizer, scheduler, device, epochs, accumulation_steps, checkpoint_path, best_model_path, load_from='last'):\n",
    "    best_val_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Decide whether to load from the last checkpoint or the best model\n",
    "    if load_from == \"last\" and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading last checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        log_density_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        last_loss = checkpoint['loss']\n",
    "        print(f\"Resumed training from epoch {start_epoch} with last loss: {last_loss:.4f}\")\n",
    "    elif load_from == \"best\" and os.path.exists(best_model_path):\n",
    "        print(f\"Loading best model from {best_model_path}\")\n",
    "        best_model_checkpoint = torch.load(best_model_path)\n",
    "        log_density_model.load_state_dict(best_model_checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(best_model_checkpoint['optimizer_state_dict'])\n",
    "        best_val_loss = best_model_checkpoint['loss']\n",
    "        print(f\"Best model loaded with validation loss: {best_val_loss:.4f}\")\n",
    "    elif load_from is not None:\n",
    "        print(f\"No checkpoint found for option '{load_from}'. Starting training from scratch.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch.\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        train_loss = train_one_epoch(train_loader, resnet, log_density_model, noise_levels, optimizer, device, scaler, accumulation_steps)\n",
    "        val_loss, avg_metrics = validate_one_epoch(val_loader, resnet, log_density_model, noise_levels, device)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Metrics - Precision: {avg_metrics[0]:.4f}, Recall: {avg_metrics[1]:.4f}, F1: {avg_metrics[2]:.4f}, IoU: {avg_metrics[3]:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        save_model(log_density_model, optimizer, epoch, val_loss, path=checkpoint_path)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(log_density_model, optimizer, epoch, val_loss, path=best_model_path)\n",
    "            print(f\"Best model updated with Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Sat Patches Shape: torch.Size([135, 3, 128, 128])\n",
      "Map Patches Shape: torch.Size([135, 128, 128])\n",
      "Non-Zero Count in Map Patches: 153000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype = None)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim = False, *, torch.dtype dtype = None)\n * (tuple of names dim, bool keepdim = False, *, torch.dtype dtype = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m non_zero_count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcount_nonzero(map_patches)  \u001b[38;5;66;03m# Counts non-zero elements\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-Zero Count in Map Patches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnon_zero_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m non_nan_count \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_patches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Counts non-NaN elements\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-NaN Count in Map Patches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnon_nan_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m plot_patches(sat_patches, map_patches, n_patches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype = None)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim = False, *, torch.dtype dtype = None)\n * (tuple of names dim, bool keepdim = False, *, torch.dtype dtype = None)\n"
     ]
    }
   ],
   "source": [
    "def plot_patches(sat_patches, map_patches, n_patches=1):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    for i in range(min(n_patches, sat_patches.shape[0])):\n",
    "        sat_patch = sat_patches[i].numpy().transpose(1, 2, 0) \n",
    "        map_patch = map_patches[i].numpy()\n",
    "\n",
    "        ax[0].imshow(sat_patch)\n",
    "        ax[0].set_title(f'Satellite Patch {i+1}')\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        ax[1].imshow(map_patch, cmap='gray')\n",
    "        ax[1].set_title(f'Map Patch {i+1}')\n",
    "        ax[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "for i, (sat_patches, map_patches) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"Sat Patches Shape: {sat_patches.shape}\")\n",
    "    print(f\"Map Patches Shape: {map_patches.shape}\")\n",
    "    non_zero_count = np.count_nonzero(map_patches)  # Counts non-zero elements\n",
    "    print(f\"Non-Zero Count in Map Patches: {non_zero_count}\")\n",
    "    non_nan_count = np.sum(~np.isnan(map_patches))  # Counts non-NaN elements\n",
    "    print(f\"Non-NaN Count in Map Patches: {non_nan_count}\")\n",
    "\n",
    "    plot_patches(sat_patches, map_patches, n_patches=1)\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▎       | 261/1108 [02:57<10:36,  1.33it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Run Training ---\n",
    "train_model(\n",
    "    resnet=resnet,\n",
    "    log_density_model=log_density_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    noise_levels=noise_levels,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    epochs=EPOCHS,\n",
    "    accumulation_steps=ACCUMULATION_STEPS,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    best_model_path=BEST_MODEL_PATH,\n",
    "    load_from=LOAD_FROM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "test_loss, test_metrics = evaluate_on_test(\n",
    "    test_loader=test_loader,\n",
    "    resnet=resnet,\n",
    "    log_density_model=log_density_model,\n",
    "    noise_levels=noise_levels,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Metrics - Precision: {test_metrics[0]:.4f}, Recall: {test_metrics[1]:.4f}, F1: {test_metrics[2]:.4f}, IoU: {test_metrics[3]:.4f}\")\n",
    "\n",
    "# --- Visualize Predictions on Validation Set ---\n",
    "val_batch = next(iter(val_loader))\n",
    "sat_patches, road_maps = val_batch\n",
    "sat_patches, road_maps = sat_patches.to(DEVICE), road_maps.to(DEVICE).float() / 255.0\n",
    "road_maps = road_maps.unsqueeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = forward_pass(sat_patches, noise_levels, resnet, log_density_model)\n",
    "    predictions = torch.sigmoid(predictions[:, 0, :, :, :])\n",
    "\n",
    "plot_predictions(predictions, road_maps, sat_patches, n_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
